{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chetxn04/Discrete_KV_Bottleneck/blob/main/NUS_Bigger_Experiment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kidwRAKq4A0l",
        "outputId": "797f9755-d446-4261-d4f3-43ee1492a013"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "# Code is yet to be comeplted. I stopped this task for the time being due to the unavaialablity fo reosurces required for trianing this model on the specified parameters.\n",
        "# This code serves the purpose of showing my progress uptil now\n",
        "# Refer to the file titles NUS_Final.ipynb in my github for a relatively lower scale implementaiton of Discrete Key Value Bottleneck.\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets, models\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "# Verifying first if gpu is available or not (shifted it to a t4 GPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# next step invovles preparing the datasets\n",
        "\n",
        "# CIFAR 10 will be the main dataset used for classification\n",
        "# CIFAR 100 will be used for initializing the keys in the KV bottleneck\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#\n",
        "def show_images(images, labels, classes, nrow=8):\n",
        "  fig,axs = plt.subplots(nrow, len(images)//nrow, figsize=(12,6))\n",
        "  axs = axs.flatten()\n",
        "  for img, label, ax in zip(images,labels,axs):\n",
        "    img = img / 2 + 0.5\n",
        "    npimg = img.numpy()\n",
        "    ax.imshow(np.transpose(npimg, (1,2,0)))\n",
        "    ax.set_title(classes[label])\n",
        "    ax.axis('off')\n",
        "  plt.tight_layout()\n",
        "  plt.show()\n",
        "\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "])\n",
        "\n",
        "# 2.2 Loading datasets\n",
        "print(\"Loading CIFAR-10 dataset....\")\n",
        "train_dataset = datasets.CIFAR10(root='./data', train=True, transform=train_transforms, download=True)\n",
        "test_dataset = datasets.CIFAR10(root='./data', train=False, transform=test_transforms, download=True)\n",
        "\n",
        "print(\"Loading CIFAR-100 dataset for key initialization..\")\n",
        "cifar100_dataset = datasets.CIFAR100(root='./data', train=True, transform=train_transforms, download=True)\n",
        "\n",
        "print(\"Creating data loaders...\")\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
        "cifar100_loader = DataLoader(dataset=cifar100_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "\n",
        "print(\"Checking batch dimensions...\")\n",
        "train_batch = next(iter(train_loader))\n",
        "cifar100_batch = next(iter(cifar100_loader))\n",
        "print(f\"CIFAR-10 training batch shape (images): {train_batch[0].shape}, labels shape: {train_batch[1].shape}\")\n",
        "print(f\"CIFAR-100 batch shape (images): {cifar100_batch[0].shape}, labels shape: {cifar100_batch[1].shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RV48OBBW6IFh",
        "outputId": "e07e4485-0fa0-4245-aed7-002e85f99a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading CIFAR-10 dataset...\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:18<00:00, 9172418.42it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n",
            "Loading CIFAR-100 dataset for key initialization...\n",
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz to ./data/cifar-100-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 169001437/169001437 [00:12<00:00, 13023655.18it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-100-python.tar.gz to ./data\n",
            "Creating data loaders...\n",
            "Checking batch dimensions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CIFAR-10 training batch shape (images): torch.Size([32, 3, 32, 32]), labels shape: torch.Size([32])\n",
            "CIFAR-100 batch shape (images): torch.Size([32, 3, 32, 32]), labels shape: torch.Size([32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import models\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.resnet18 = models.resnet18(pretrained=True)  # Used ResNet-18 as it is lighter\n",
        "        self.resnet18.fc = nn.Linear(self.resnet18.fc.in_features, 512)\n",
        "\n",
        "    def forward(self, x):\n",
        "        features = self.resnet18(x)  # Forward pass through ResNet-18\n",
        "        return features\n"
      ],
      "metadata": {
        "id": "v-h174Aa727l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def generate_random_projection_matrices(input_dim, output_dim, num_codebooks):\n",
        "    # Generates a list of fixed Gaussian projection matrices for each codebook\n",
        "    matrices = [torch.randn(output_dim, input_dim) for _ in range(num_codebooks)]\n",
        "    return matrices\n",
        "\n",
        "class DiscreteKeyValueBottleneck(nn.Module):\n",
        "    def __init__(self, num_codebooks=4, num_keys=15, key_dim=512, value_dim=512):\n",
        "        super(DiscreteKeyValueBottleneck, self).__init__()\n",
        "        self.num_codebooks = num_codebooks\n",
        "        self.num_keys = num_keys\n",
        "\n",
        "        # Initialize keys and values for each codebook\n",
        "        reduced_dim = key_dim // num_codebooks  # Projected dimension\n",
        "        self.keys = nn.Parameter(torch.randn(num_codebooks, num_keys, reduced_dim), requires_grad=False)  # Keys are frozen\n",
        "        self.values = nn.Parameter(torch.randn(num_codebooks, num_keys, value_dim))\n",
        "\n",
        "        # Initialize EMA for keys\n",
        "        self.ema_keys = torch.zeros_like(self.keys)\n",
        "\n",
        "        # Generate fixed Gaussian projection matrices once and store them for use later during inputs\n",
        "        self.projection_matrices = generate_random_projection_matrices(key_dim, reduced_dim, num_codebooks)\n",
        "\n",
        "    def forward(self, features):\n",
        "        features = F.normalize(features, dim=1)\n",
        "\n",
        "        projected_features = [features @ self.projection_matrices[i].t().to(features.device) for i in range(self.num_codebooks)]\n",
        "\n",
        "        distances = torch.stack([torch.cdist(proj_feat.unsqueeze(1), self.keys[i].unsqueeze(0)).squeeze(1)\n",
        "                                 for i, proj_feat in enumerate(projected_features)], dim=1)\n",
        "\n",
        "        # FindING the closest key in each codebook\n",
        "        _, indices = torch.min(distances, dim=2)\n",
        "        selected_values = torch.stack([self.values[i][indices[:, i]] for i in range(self.num_codebooks)], dim=1)\n",
        "\n",
        "        # Average the selected values from all codebooks\n",
        "        combined_values = selected_values.mean(dim=1)\n",
        "\n",
        "        return combined_values, indices\n",
        "\n",
        "    def update_keys(self, new_keys, codebook_idx):\n",
        "\n",
        "        new_keys = new_keys.to(self.ema_keys.device) # Ensure new_keys is on the same device as ema_keys (Credit to GPT for this tip)\n",
        "\n",
        "        self.ema_keys[codebook_idx] = 0.9 * self.ema_keys[codebook_idx] + 0.1 * new_keys\n",
        "        self.keys.data[codebook_idx].copy_(self.ema_keys[codebook_idx])  # Copy EMA to keys\n",
        "\n",
        "    def print_gaussian_matrices(self):\n",
        "        # Print the Gaussian projection matrices for visual verification\n",
        "        for idx, matrix in enumerate(self.projection_matrices):\n",
        "            print(f\"Gaussian Projection Matrix for Codebook {idx}:\")\n",
        "            print(matrix)\n",
        "\n",
        "def initialize_keys_with_cifar100(encoder, bottleneck, num_codebooks=4, num_keys=15, device='cpu'):\n",
        "    transform = transforms.Compose([transforms.ToTensor()])\n",
        "    cifar100_data = datasets.CIFAR100(root='./data', train=True, download=True, transform=transform)\n",
        "    cifar100_loader = DataLoader(cifar100_data, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "    features_list = []\n",
        "\n",
        "    # Pass through the encoder to get features\n",
        "    with torch.no_grad():\n",
        "        for images, _ in tqdm(cifar100_loader, desc=\"Extracting Features\"):\n",
        "            images = images.to(device)\n",
        "            features = encoder(images).cpu()  # Moving features to CPU\n",
        "            features_list.append(features)\n",
        "\n",
        "    all_features = torch.cat(features_list, dim=0).numpy()  # Convert to NumPy for KMeans\n",
        "\n",
        "    # Use Gaussian projection and KMeans clustering to initialize keys for each codebook\n",
        "    for i in range(num_codebooks):\n",
        "\n",
        "        projected_features = all_features @ bottleneck.projection_matrices[i].t().numpy()\n",
        "\n",
        "        # Perform KMeans clustering on the projected features\n",
        "        kmeans = KMeans(n_clusters=num_keys, random_state=42 + i)\n",
        "        kmeans.fit(projected_features)\n",
        "        initial_keys = torch.tensor(kmeans.cluster_centers_, device=device)\n",
        "\n",
        "        # Update keys using EMA\n",
        "        bottleneck.update_keys(initial_keys, i)\n",
        "\n",
        "    return bottleneck\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "set_seed(42)\n",
        "\n",
        "encoder = Encoder().to(device)\n",
        "bottleneck = DiscreteKeyValueBottleneck(num_codebooks=4, num_keys=15, key_dim=512, value_dim=512).to(device)\n",
        "bottleneck = initialize_keys_with_cifar100(encoder, bottleneck, num_codebooks=4, num_keys=15, device=device)\n",
        "\n",
        "bottleneck.print_gaussian_matrices()\n",
        "\n",
        "print(\"Keys after initialization with CIFAR-100 using clustering:\")\n",
        "print(bottleneck.keys)\n"
      ],
      "metadata": {
        "id": "NZ_k1WG7QNWK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2d02926-e053-487c-d988-4025bc2ca11a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Features: 100%|██████████| 1563/1563 [00:18<00:00, 84.36it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Projection Matrix for Codebook 0:\n",
            "tensor([[-0.7105, -0.0198, -0.0777,  ...,  2.2460,  3.0764,  0.0519],\n",
            "        [-0.6180, -0.2051,  1.4381,  ..., -0.4459,  0.5250, -1.2413],\n",
            "        [-1.0603, -0.3556,  0.9289,  ...,  0.1741, -0.9815,  1.9251],\n",
            "        ...,\n",
            "        [ 0.3850, -0.8261, -1.4157,  ..., -0.2275, -0.5870, -1.1628],\n",
            "        [-0.5510,  0.4117, -0.5996,  ...,  0.4430,  0.0566, -0.6701],\n",
            "        [-0.6224,  1.6981, -0.4885,  ...,  0.7364, -1.0163, -1.0466]])\n",
            "Gaussian Projection Matrix for Codebook 1:\n",
            "tensor([[ 0.1659,  0.6571,  1.5753,  ...,  0.6519, -0.3043, -2.1921],\n",
            "        [-0.7356, -1.1633, -0.1480,  ...,  0.8037, -0.3714, -0.1970],\n",
            "        [-0.5265, -0.3033, -0.1243,  ...,  0.2786, -0.8021, -1.7504],\n",
            "        ...,\n",
            "        [-2.2782, -1.0480, -0.4143,  ...,  1.2378,  0.4139, -1.2492],\n",
            "        [-2.5349,  0.6819,  0.5568,  ...,  0.7243,  0.3141,  1.2299],\n",
            "        [-0.8147, -1.2754,  1.7540,  ...,  0.7978, -1.7152, -0.1408]])\n",
            "Gaussian Projection Matrix for Codebook 2:\n",
            "tensor([[-0.3180, -1.0116, -0.7467,  ...,  1.1536, -1.9659,  0.8797],\n",
            "        [ 0.0660,  1.5319,  0.0883,  ...,  0.0452, -0.1603,  0.5564],\n",
            "        [ 0.1083,  0.2698, -0.9396,  ...,  1.9473, -0.3415, -1.1753],\n",
            "        ...,\n",
            "        [-0.2402,  0.9415, -0.3603,  ...,  0.3131,  0.1088, -0.8583],\n",
            "        [ 2.4843,  0.0817, -0.6600,  ...,  2.1911, -1.1997,  0.7281],\n",
            "        [-0.3094, -0.4174,  0.6698,  ..., -0.5258,  1.1220, -0.9961]])\n",
            "Gaussian Projection Matrix for Codebook 3:\n",
            "tensor([[-1.6050, -0.5494, -1.1409,  ...,  1.9570, -0.8091, -1.4036],\n",
            "        [-1.3261, -0.4742, -0.6827,  ..., -0.9285, -0.4525, -0.3251],\n",
            "        [ 0.9723, -2.0395, -1.0479,  ..., -1.1035, -1.1807,  0.0747],\n",
            "        ...,\n",
            "        [-0.1113, -0.7005,  0.6093,  ..., -0.1717,  0.2387, -0.1792],\n",
            "        [ 2.4346,  0.1285, -0.6121,  ...,  0.9159,  0.0070,  0.2595],\n",
            "        [-0.0602,  0.2290,  0.0169,  ...,  0.7454,  0.2515, -0.8634]])\n",
            "Keys after initialization with CIFAR-100 using clustering:\n",
            "Parameter containing:\n",
            "tensor([[[ 0.5248, -0.7983,  0.0593,  ..., -1.4518, -2.5975, -1.5833],\n",
            "         [-0.9009,  0.3050, -1.3559,  ..., -1.1628, -1.9692, -0.1838],\n",
            "         [ 0.2626, -0.3487, -0.1463,  ...,  0.2548, -0.5072, -2.1045],\n",
            "         ...,\n",
            "         [-0.5629, -1.3833, -0.9871,  ...,  0.5442, -3.1701, -1.6317],\n",
            "         [ 0.3272, -1.2472, -1.2406,  ..., -0.3849, -2.2236, -1.4601],\n",
            "         [-0.6260, -1.4564, -0.4675,  ..., -0.1679, -1.8075, -1.3519]],\n",
            "\n",
            "        [[ 0.2617, -0.9652,  0.3771,  ...,  1.2523,  1.1564, -0.0701],\n",
            "         [ 1.7864,  0.7508,  0.1193,  ...,  1.3009,  1.1990, -0.4493],\n",
            "         [ 0.6654, -0.0032,  0.6851,  ...,  0.6277,  1.4820, -0.3889],\n",
            "         ...,\n",
            "         [ 2.1348,  1.1637,  2.0307,  ...,  1.3059,  1.1911, -0.2124],\n",
            "         [ 0.5000, -1.1232,  2.8189,  ...,  2.9962,  1.7865, -0.7910],\n",
            "         [ 0.3898, -0.4472,  0.7621,  ...,  1.4626,  2.3237,  0.0782]],\n",
            "\n",
            "        [[-1.0800,  0.0343,  0.4555,  ..., -1.8926, -0.7539,  0.8123],\n",
            "         [-0.9378,  0.7191,  0.2762,  ..., -2.1915, -0.8387,  0.3112],\n",
            "         [-1.8659,  0.4756,  0.8241,  ..., -2.4077, -0.7419,  0.0995],\n",
            "         ...,\n",
            "         [-2.2860,  0.3671,  0.0202,  ..., -2.5410, -2.3518, -0.0550],\n",
            "         [-0.7173,  0.1029, -0.8052,  ..., -2.8543, -0.1404,  0.5491],\n",
            "         [-1.4440, -1.5311, -3.2268,  ...,  1.3703, -1.8332, -0.6926]],\n",
            "\n",
            "        [[ 0.8955, -1.2060,  0.6503,  ...,  1.0716,  2.0597,  0.0483],\n",
            "         [ 0.1945, -0.6949, -0.0918,  ...,  0.0598,  1.3226,  0.2999],\n",
            "         [-0.7748, -1.5218, -0.1277,  ..., -0.0071,  2.0191,  0.5282],\n",
            "         ...,\n",
            "         [ 0.4273, -1.7343,  0.6247,  ...,  1.7831,  2.0148,  2.2890],\n",
            "         [-0.4301, -1.4091,  0.4283,  ..., -0.4332,  1.4175, -0.2226],\n",
            "         [ 0.9918, -1.3016, -0.9438,  ..., -1.3125, -0.1158,  0.7574]]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visual verificaiton code (Not important)\n",
        "# import pandas as pd\n",
        "\n",
        "# def display_keys_and_values(bottleneck):\n",
        "#     # Convert keys and values to NumPy arrays\n",
        "#     keys_np = bottleneck.keys.detach().cpu().numpy()\n",
        "#     values_np = bottleneck.values.detach().cpu().numpy()\n",
        "\n",
        "#     # Reshape keys and values for easier display\n",
        "#     num_codebooks, num_keys, key_dim = keys_np.shape\n",
        "#     _, _, value_dim = values_np.shape\n",
        "\n",
        "#     # Flatten the keys and values while keeping the codebook index\n",
        "#     keys_flattened = keys_np.reshape(num_codebooks * num_keys, key_dim)\n",
        "#     values_flattened = values_np.reshape(num_codebooks * num_keys, value_dim)\n",
        "\n",
        "#     # Create DataFrames for keys and values\n",
        "#     keys_df = pd.DataFrame(keys_flattened, columns=[f'Key_Dim_{i}' for i in range(key_dim)])\n",
        "#     values_df = pd.DataFrame(values_flattened, columns=[f'Value_Dim_{i}' for i in range(value_dim)])\n",
        "#     codebook_df = pd.DataFrame({'Codebook': [f'Codebook_{i // num_keys}' for i in range(num_codebooks * num_keys)]})\n",
        "\n",
        "#     # Concatenate the DataFrames along the columns\n",
        "#     df = pd.concat([codebook_df, keys_df, values_df], axis=1)\n",
        "\n",
        "#     # Display the DataFrame\n",
        "#     print(\"Keys and Corresponding Values in Tabular Format:\")\n",
        "#     print(df)\n",
        "\n",
        "# # Example usage\n",
        "# display_keys_and_values(bottleneck)\n"
      ],
      "metadata": {
        "id": "tjma5YTLQ17i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54b7b1f1-2fb0-4e74-d888-b9f555ca41df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys and Corresponding Values in Tabular Format:\n",
            "      Codebook  Key_Dim_0  Key_Dim_1  Key_Dim_2  Key_Dim_3  Key_Dim_4  \\\n",
            "0   Codebook_0   0.524784  -0.798257   0.059302   1.957101   4.585316   \n",
            "1   Codebook_0  -0.900870   0.304977  -1.355916  -1.029992   2.378527   \n",
            "2   Codebook_0   0.262564  -0.348699  -0.146293  -1.154292   3.075608   \n",
            "3   Codebook_0  -0.440521  -0.908660  -0.933542   0.570259   1.705456   \n",
            "4   Codebook_0   0.526088  -2.328063  -1.483508   1.050314   1.978939   \n",
            "5   Codebook_0  -0.423469   0.048299  -0.646480   1.040738   2.162914   \n",
            "6   Codebook_0   0.139213  -0.881782  -1.381922  -0.757783   0.144770   \n",
            "7   Codebook_0  -2.110939  -0.358238  -0.654760   0.604697   1.642229   \n",
            "8   Codebook_0  -1.522163  -0.688197  -0.870066  -0.722033   1.638394   \n",
            "9   Codebook_0  -0.062502  -0.486753  -0.429995  -0.080553   1.425506   \n",
            "10  Codebook_0   0.050209  -2.076406  -1.023960  -0.318207   1.209471   \n",
            "11  Codebook_0  -0.884760  -0.797551  -0.839556   0.469589   1.770886   \n",
            "12  Codebook_0  -0.562917  -1.383317  -0.987115  -1.517205   3.274063   \n",
            "13  Codebook_0   0.327158  -1.247164  -1.240606   0.445636   2.267502   \n",
            "14  Codebook_0  -0.626042  -1.456388  -0.467526  -0.521868   2.059996   \n",
            "15  Codebook_1   0.261703  -0.965208   0.377110  -0.315310  -0.226316   \n",
            "16  Codebook_1   1.786377   0.750844   0.119349  -0.406698  -0.388439   \n",
            "17  Codebook_1   0.665428  -0.003243   0.685103  -0.144683  -0.020929   \n",
            "18  Codebook_1   2.267712  -1.467407   0.233050  -0.584129   0.071343   \n",
            "19  Codebook_1   0.444340  -1.183657  -0.315540  -1.110907  -0.273828   \n",
            "20  Codebook_1   2.280007   1.748266  -0.432936  -1.438016   0.063936   \n",
            "21  Codebook_1   0.588843  -1.854427   0.358289  -0.118117   1.026882   \n",
            "22  Codebook_1   0.373535   0.017381  -0.090939  -0.956656  -1.105338   \n",
            "23  Codebook_1   1.007269  -0.680720   1.146153  -1.071068   0.105829   \n",
            "24  Codebook_1   0.453679  -0.881331  -0.795722  -1.073894  -1.210654   \n",
            "25  Codebook_1   2.632880  -0.024391   2.240552  -1.350836   1.173597   \n",
            "26  Codebook_1   1.982146   0.098458   0.480274  -0.605779   0.009798   \n",
            "27  Codebook_1   2.134779   1.163681   2.030694  -0.600048   0.287586   \n",
            "28  Codebook_1   0.499978  -1.123199   2.818913   0.104355   1.757518   \n",
            "29  Codebook_1   0.389784  -0.447189   0.762102  -0.350527   0.538027   \n",
            "30  Codebook_2  -1.079962   0.034252   0.455546   0.750528   0.170357   \n",
            "31  Codebook_2  -0.937848   0.719141   0.276193   0.951811  -0.364539   \n",
            "32  Codebook_2  -1.865901   0.475585   0.824112   0.516156   0.277020   \n",
            "33  Codebook_2  -2.681717   1.400313   0.998146   0.137787   0.961712   \n",
            "34  Codebook_2  -0.791708  -0.364767   0.477711   0.298005  -0.244792   \n",
            "35  Codebook_2  -0.489945   0.019620   0.241093   0.779951   0.355160   \n",
            "36  Codebook_2  -1.921411  -1.719076  -1.766286  -0.668371   0.246332   \n",
            "37  Codebook_2  -1.435996  -1.146722  -0.406113   0.457688  -0.001264   \n",
            "38  Codebook_2  -0.979475   0.369144   0.010681   0.772574   0.329844   \n",
            "39  Codebook_2  -3.293281  -0.705637  -1.128812  -1.008450   1.434553   \n",
            "40  Codebook_2  -1.770121   0.669812   0.142833   1.752636   1.087121   \n",
            "41  Codebook_2  -1.105585  -0.010653   0.371665   1.236201   1.646098   \n",
            "42  Codebook_2  -2.286019   0.367144   0.020155   1.173396   0.705939   \n",
            "43  Codebook_2  -0.717268   0.102866  -0.805180   1.334809   0.173870   \n",
            "44  Codebook_2  -1.443991  -1.531095  -3.226752  -1.305869   0.029712   \n",
            "45  Codebook_3   0.895514  -1.206015   0.650322   1.604033  -2.027456   \n",
            "46  Codebook_3   0.194452  -0.694924  -0.091832   0.671295  -0.874338   \n",
            "47  Codebook_3  -0.774793  -1.521819  -0.127750   0.984103  -0.558498   \n",
            "48  Codebook_3  -0.843512  -2.402834  -0.215771   0.517722  -0.819006   \n",
            "49  Codebook_3   0.826268  -1.942985  -0.057859   1.428013  -0.659971   \n",
            "50  Codebook_3  -0.060894  -1.691524  -0.493802   1.337819  -1.021132   \n",
            "51  Codebook_3   0.576076  -0.498111  -1.078692   0.389992  -1.364293   \n",
            "52  Codebook_3  -0.668399  -1.834272  -1.402750   2.158801   0.523047   \n",
            "53  Codebook_3   0.104865  -2.065762  -0.458950  -0.294812  -1.037597   \n",
            "54  Codebook_3   0.026588  -1.875534   0.183854   0.989694  -1.543502   \n",
            "55  Codebook_3   1.884311  -1.579237   0.217523   1.771234  -2.157563   \n",
            "56  Codebook_3   1.496771  -0.752449   0.415610   0.946579  -2.367189   \n",
            "57  Codebook_3   0.427330  -1.734310   0.624656   0.342976  -1.168359   \n",
            "58  Codebook_3  -0.430078  -1.409145   0.428343   1.219967  -0.210060   \n",
            "59  Codebook_3   0.991768  -1.301595  -0.943772  -0.454370  -4.815568   \n",
            "\n",
            "    Key_Dim_5  Key_Dim_6  Key_Dim_7  Key_Dim_8  ...  Value_Dim_502  \\\n",
            "0   -1.102546  -2.922299  -2.168532  -1.795160  ...      -1.312504   \n",
            "1    1.120796  -0.296076  -1.193619  -0.289165  ...       0.395885   \n",
            "2   -0.096594  -3.215774  -1.078030   1.124778  ...       1.425243   \n",
            "3    0.112670  -2.147323  -1.642381  -0.209299  ...      -3.105919   \n",
            "4    0.442692  -2.192476  -1.339941   0.467717  ...      -0.093106   \n",
            "5    1.086592  -2.539568  -1.757120  -0.671593  ...      -1.236447   \n",
            "6    1.027381  -0.659651  -1.969354   0.042933  ...      -0.911637   \n",
            "7    0.342648  -0.998711  -2.185898   0.880462  ...      -0.232241   \n",
            "8   -0.736647  -3.448748  -1.987645   0.129247  ...      -0.975482   \n",
            "9    0.570063  -1.465968  -0.952090   0.374649  ...      -0.070261   \n",
            "10   1.075312  -1.289444  -1.907331  -1.563337  ...      -1.362767   \n",
            "11   0.287143  -1.440398  -2.790894   0.565491  ...       0.957104   \n",
            "12  -0.487695  -1.767821  -0.890052   1.339093  ...      -0.287814   \n",
            "13  -1.674525  -1.772665  -2.225711  -0.289265  ...      -0.875497   \n",
            "14   0.291592  -1.314196  -1.218796   0.660716  ...      -0.046653   \n",
            "15   1.384457  -0.210973   0.668170   0.243976  ...      -0.581348   \n",
            "16   0.548980  -0.133747  -0.977318  -0.180015  ...       2.101712   \n",
            "17   0.542633   0.359858  -0.235042  -0.628337  ...       0.181769   \n",
            "18   0.802677   1.987084   0.100091  -0.592327  ...      -0.044265   \n",
            "19   1.745652   0.904846   1.844450  -0.610683  ...      -0.711810   \n",
            "20   1.251724   0.730446  -0.568320  -0.771975  ...       1.307022   \n",
            "21   0.610036   2.186363  -0.151457   0.184975  ...       0.184644   \n",
            "22   1.090845  -0.653950   0.899545   1.611731  ...      -0.849662   \n",
            "23   1.549246   0.569379   0.219403  -0.142674  ...       0.085954   \n",
            "24   1.132005   0.044598  -0.959455   0.699760  ...       0.245499   \n",
            "25   1.219427   0.757333  -0.510644  -0.357260  ...       0.352597   \n",
            "26   0.444780   0.908943  -0.694836  -0.638699  ...      -0.374496   \n",
            "27   0.002804   0.927292  -0.136222   0.529840  ...       1.454500   \n",
            "28   1.817758  -0.333968  -0.292407   1.164956  ...      -0.696175   \n",
            "29   0.331069   0.075590  -0.107077  -1.419701  ...      -0.280199   \n",
            "30   1.318362   1.186703  -0.274475  -1.045615  ...      -0.677281   \n",
            "31   1.407311   0.593883  -0.973743  -0.369649  ...       0.494596   \n",
            "32   2.436977  -0.411339  -1.492349  -0.630906  ...       0.841078   \n",
            "33   3.502396  -0.080321  -1.048125  -0.638322  ...      -0.457357   \n",
            "34   1.659747   0.358338  -0.793494  -1.408352  ...       0.986035   \n",
            "35   1.322815   1.123940  -0.186490  -1.011498  ...      -1.173697   \n",
            "36   2.701243   0.261880  -0.689938  -2.326933  ...       0.225653   \n",
            "37   2.168091   0.886518  -0.809495  -0.382336  ...       0.185281   \n",
            "38   1.603097   0.189711  -0.256310  -0.054885  ...      -0.352091   \n",
            "39   1.678797   1.301363   1.136368  -1.143140  ...       0.638944   \n",
            "40   0.595422  -0.444869   0.355277  -0.346718  ...       1.820686   \n",
            "41   1.415074  -0.141741  -0.377255  -0.177344  ...      -0.370676   \n",
            "42   2.408420  -0.326668  -0.678038  -0.864093  ...      -1.074322   \n",
            "43   1.805521   1.139113  -0.226430  -0.700582  ...      -0.556725   \n",
            "44   2.809512   0.357200   0.378336  -0.904957  ...       0.893088   \n",
            "45  -2.035012  -1.342095   0.270840  -1.210282  ...       0.365950   \n",
            "46  -0.487284  -0.482776  -0.390792  -0.696032  ...      -0.379145   \n",
            "47  -1.104571  -1.182445  -0.887325  -1.084361  ...      -2.972296   \n",
            "48   0.519830   0.120138  -1.105568  -1.718936  ...       1.045625   \n",
            "49  -0.619423  -1.371591  -0.817355  -0.410026  ...      -0.717495   \n",
            "50  -1.566675   1.224666  -0.642322  -0.044522  ...      -1.182699   \n",
            "51  -0.622934  -0.183325  -1.691015  -0.050796  ...      -0.091574   \n",
            "52  -2.579748  -1.025373  -0.570255  -2.063949  ...      -0.471142   \n",
            "53  -1.468002  -1.325595  -0.808171  -1.857900  ...       0.739514   \n",
            "54  -1.328146  -1.654753  -0.491318  -0.862163  ...      -0.888509   \n",
            "55  -1.417939  -1.531945  -1.238595  -1.168613  ...       0.867045   \n",
            "56  -1.340818  -0.859683  -1.461972   1.293465  ...      -0.005499   \n",
            "57  -2.621723  -0.265930  -0.328344  -1.608405  ...       0.645311   \n",
            "58  -1.008841  -1.805851  -0.647623  -0.772399  ...       0.844987   \n",
            "59   0.377278  -0.684968  -0.048337   0.245162  ...      -0.816782   \n",
            "\n",
            "    Value_Dim_503  Value_Dim_504  Value_Dim_505  Value_Dim_506  Value_Dim_507  \\\n",
            "0        1.347579       0.462122       0.965399      -0.047643       0.700658   \n",
            "1       -0.328476      -0.220578      -0.345848       0.840039       0.174160   \n",
            "2       -0.221410      -1.628824       0.182268      -1.157188      -0.502274   \n",
            "3       -1.866449      -0.935934       1.059726       0.163632       1.319282   \n",
            "4       -0.357598       0.710351      -0.311501       1.005944       0.434763   \n",
            "5        0.169520       0.414010       1.413524       1.184200      -0.641171   \n",
            "6        0.633976       1.236065       0.709625      -2.299632      -1.490271   \n",
            "7       -0.816197       0.672538      -0.527431       1.082287      -0.217390   \n",
            "8       -1.130741      -1.020189      -0.711343      -1.796543       0.990764   \n",
            "9       -0.587416       0.300779      -0.294343       0.287792      -0.063759   \n",
            "10       0.117753      -0.750802       1.168361      -0.592430       2.653559   \n",
            "11       0.699928      -0.885135      -0.166856       1.287632      -1.315949   \n",
            "12       0.199344      -0.394441      -1.272367       0.694822       0.435382   \n",
            "13       0.521024       1.408344      -0.147385       0.742180       0.805734   \n",
            "14      -0.486894       1.462088       2.214452       0.912402      -0.331818   \n",
            "15       0.725148      -0.945228       0.702487       2.405474      -0.028553   \n",
            "16       0.211141       0.664722       0.490602      -0.323543      -0.465364   \n",
            "17      -0.618905      -0.256985       2.156208       1.429413      -1.635902   \n",
            "18      -0.461031      -1.132334      -0.356472       1.852660      -0.107684   \n",
            "19       0.418853      -0.289407       1.553569      -1.616911      -1.096258   \n",
            "20      -1.376402      -0.976481       0.415323       2.431226       0.195116   \n",
            "21       0.448922       0.850530      -0.531604       2.651114       1.169211   \n",
            "22      -0.656659      -0.483649       1.437360      -0.868190       1.009568   \n",
            "23       0.711575       0.859352      -0.355059      -1.976998       1.375954   \n",
            "24      -1.346511       1.914268       0.167689      -0.545258      -0.044893   \n",
            "25       0.766541       1.151263      -0.911208       1.033147       0.586799   \n",
            "26      -0.926825      -0.968953      -1.098220      -1.673558      -1.421253   \n",
            "27       1.508232       0.225501      -0.295014       0.092625       1.044478   \n",
            "28      -0.585926      -0.548057       0.578944       0.360452      -1.963379   \n",
            "29       1.329999      -0.764298      -1.739207       0.818134       0.063360   \n",
            "30      -1.414347       0.617238       0.588108       0.096647      -0.321199   \n",
            "31       1.214242       0.310467       1.027853       1.015842       2.151988   \n",
            "32      -0.432939      -1.535153      -0.140614      -1.219613      -0.635448   \n",
            "33      -0.422820       1.453694      -0.692504       0.397852       1.883696   \n",
            "34      -0.867801      -0.299617      -0.259733      -1.443650      -0.113303   \n",
            "35       1.674890       0.443845      -1.006716       0.178322       0.661143   \n",
            "36      -1.164922      -0.411945       0.260337      -0.119990       0.070973   \n",
            "37      -0.321358      -0.699958      -0.748684       0.102773       0.821474   \n",
            "38      -0.910436      -1.080245      -0.830494      -0.442849      -0.521997   \n",
            "39       0.402310       0.775078      -1.649517       1.039069      -0.059975   \n",
            "40       1.624789      -0.621273       0.370025      -0.333261      -0.332271   \n",
            "41      -0.677617      -0.211403      -0.444549       1.548583       0.082286   \n",
            "42      -1.161984      -2.064665       0.381123      -0.538930      -0.843853   \n",
            "43       0.412806       1.097825       1.119380       0.933365      -0.432815   \n",
            "44       0.932976      -0.031057       0.610526       0.372576      -0.276968   \n",
            "45      -1.554573      -0.817321       0.557173      -1.738159      -0.191585   \n",
            "46      -0.534063       1.989278       0.936089      -1.484949      -0.430918   \n",
            "47       0.776713      -1.464371      -0.345922      -1.584727       1.248699   \n",
            "48      -0.208363      -1.509134       0.601288      -1.151539       1.165846   \n",
            "49       0.017039      -0.048321      -0.173358       0.461198       0.079158   \n",
            "50      -0.547968      -1.253344       0.406483       0.603723      -0.435212   \n",
            "51      -0.110752      -0.245852       0.677677       0.509425       0.475456   \n",
            "52      -0.544166      -0.521361       0.278356      -0.456112       1.206265   \n",
            "53       0.708812       0.985145       1.511758      -0.328391       1.255666   \n",
            "54      -2.275211       0.473536      -0.400241      -0.090109       0.452352   \n",
            "55       0.457927      -0.454667       0.557007       0.797296      -1.438502   \n",
            "56      -1.449101       1.784873       0.360642       0.134948      -0.270864   \n",
            "57       0.280159      -0.484028      -1.389274      -0.774320       0.561926   \n",
            "58       0.462680       0.068217       0.275574      -0.215338      -0.491964   \n",
            "59      -1.374617      -0.709576      -1.659265       0.120516      -0.507082   \n",
            "\n",
            "    Value_Dim_508  Value_Dim_509  Value_Dim_510  Value_Dim_511  \n",
            "0        0.953100       0.103569       0.362945      -0.471356  \n",
            "1       -0.046699       0.773050       0.195584      -0.501245  \n",
            "2       -1.037998       0.735147      -0.429635       0.059318  \n",
            "3        0.388501       0.079926      -0.318736       0.360996  \n",
            "4       -2.015259       0.521894       0.317218      -0.059792  \n",
            "5       -0.378696      -2.153335      -2.510521      -0.897412  \n",
            "6        2.719801      -0.954423       0.515632       0.728475  \n",
            "7       -0.611573      -0.941490       0.085359       1.491742  \n",
            "8       -1.771163       0.120525       0.019959      -1.820639  \n",
            "9       -0.215135      -0.236394      -0.057332       2.431113  \n",
            "10      -1.083784       0.367389       1.063550      -1.119893  \n",
            "11      -0.226494      -0.253437       0.864803       1.105239  \n",
            "12      -0.002763       0.030461       0.303618       1.062790  \n",
            "13      -1.580588       0.938648      -1.160421      -0.412955  \n",
            "14       0.532045       0.461814      -1.323047      -1.852469  \n",
            "15      -0.344952       1.095153       1.112481       0.873488  \n",
            "16       0.531633       1.334232      -0.207943      -0.205292  \n",
            "17      -0.171043       0.987248      -0.150326      -0.433878  \n",
            "18      -0.280328      -0.995927      -0.912709      -0.458247  \n",
            "19      -1.605265       1.268189      -0.486492       0.964191  \n",
            "20       0.150643      -0.831080      -0.689587       0.621983  \n",
            "21       1.160058       0.954310       1.379222      -1.465016  \n",
            "22       1.534160       0.045317       0.170324      -1.756397  \n",
            "23       0.284760      -1.186134      -2.455581      -1.877189  \n",
            "24       0.990811      -1.191869       1.125435       1.546671  \n",
            "25       1.038974      -0.738041       0.119986       1.135909  \n",
            "26      -0.261849      -2.006142      -0.053176       1.320546  \n",
            "27       0.511493      -0.530649      -1.790918      -1.408600  \n",
            "28      -0.488709       0.642821       0.487029      -1.378049  \n",
            "29       0.720654      -0.335677      -0.139483       1.498034  \n",
            "30       1.237949       1.010683       1.987785      -0.910555  \n",
            "31      -1.045609       0.681585      -1.194958      -0.268224  \n",
            "32       0.699410      -0.908211       0.907823       0.922184  \n",
            "33       0.694412      -0.791735       1.368372       0.297075  \n",
            "34      -0.211619       1.423007       0.297808       0.411221  \n",
            "35       1.016627       1.069228      -0.948923      -0.229810  \n",
            "36       0.416217       2.202837      -0.644551      -2.079566  \n",
            "37      -0.251579      -1.745435      -0.666103      -0.483299  \n",
            "38       0.065099       0.319952      -1.562055       0.539103  \n",
            "39       0.435218       1.824665      -0.363178      -0.064899  \n",
            "40       0.187530      -0.861006      -0.108651      -0.908136  \n",
            "41       1.462147      -0.236170       0.324727       1.687390  \n",
            "42       0.172177      -1.321010       0.979725      -1.683446  \n",
            "43       0.287476      -0.631292      -0.048269      -0.786722  \n",
            "44      -0.449679       1.768116       1.035113      -1.222583  \n",
            "45       0.498352      -0.451590       0.350084       1.657145  \n",
            "46      -0.454699      -0.531400      -0.325584      -0.511152  \n",
            "47       0.507063      -1.635391       0.534563       1.476773  \n",
            "48       0.984006       0.678953      -1.415251      -2.478656  \n",
            "49       0.234801      -0.713488      -2.077489       1.203818  \n",
            "50       0.901308       1.501431      -1.253141      -1.813203  \n",
            "51       0.150498       1.321875      -0.112804      -0.183625  \n",
            "52       2.632276       2.124264      -0.540497      -0.694160  \n",
            "53      -1.205234       0.173714      -0.690692      -0.847390  \n",
            "54       2.012212      -1.241233      -0.973116       1.553865  \n",
            "55      -0.289243      -2.049071       0.278234       2.012156  \n",
            "56       0.127038      -0.347300       0.573167      -1.684888  \n",
            "57      -1.204582      -0.096938      -0.281257      -2.353664  \n",
            "58       0.427265      -0.825977       1.216386       0.741174  \n",
            "59      -0.148287       0.251760       0.792509       0.759819  \n",
            "\n",
            "[60 rows x 641 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, input_dim=512, num_classes=10):\n",
        "        super(Decoder, self).__init__()\n",
        "        # Fully connected layer to output class scores\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply the fully connected layer\n",
        "        x = self.fc(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "a079rYLDVbbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CompleteModel(nn.Module):\n",
        "    def __init__(self, encoder, bottleneck, decoder):\n",
        "        super(CompleteModel, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.bottleneck = bottleneck\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Passing input through the encoder to get features\n",
        "        features = self.encoder(x)  # Encoder output shape: [batch_size, 512]\n",
        "\n",
        "        # Passing the  features through the bottleneck to get combined values\n",
        "        combined_values, indices = self.bottleneck(features)  # Combined values shape: [batch_size, value_dim]\n",
        "\n",
        "        logits = self.decoder(combined_values)  # Logits are unnormalzied final scores (GPT info)\n",
        "\n",
        "        return logits, indices\n",
        "\n"
      ],
      "metadata": {
        "id": "Nypj1n_efvBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Code is yet to be comeplted. I stopped this task for the time being due to the unavaialablity fo reosurces required for trianing this model on the specified parameters.\n",
        "# This code serves the purpose of showing my progress uptil now\n",
        "# Refer to the file titles NUS_Final.ipynb in my github for a relatively lower scale implementaiton of Discrete Key Value Bottleneck.\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "encoder = Encoder().to(device)  # Create the encoder on the GPU\n",
        "bottleneck = DiscreteKeyValueBottleneck(num_codebooks=4, num_keys=15, key_dim=512, value_dim=512).to(device)\n",
        "decoder = Decoder(input_dim=512, num_classes=10).to(device)  # Adjust input_dim as per your bottleneck output\n",
        "\n",
        "complete_model = CompleteModel(encoder, bottleneck, decoder).to(device)\n",
        "\n",
        "num_epochs = 2000\n",
        "learning_rate = 0.005\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(complete_model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    complete_model.train()\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        logits, indices = complete_model(images)\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    average_loss = running_loss / len(train_loader)\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {average_loss:.4f}')\n",
        "\n",
        "torch.save(complete_model.state_dict(), 'complete_model.pth')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "QT3JqpQemDD6",
        "outputId": "a8080563-80ad-4c92-fb21-ca94da40ba39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/200], Loss: 2.3557\n",
            "Epoch [2/200], Loss: 2.3006\n",
            "Epoch [3/200], Loss: 2.2775\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-444e77dfe2d3>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mrunning_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m  \u001b[0;31m# Initialize loss for the epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Iterate over batches of data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move to the appropriate device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1291\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1292\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1293\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1294\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1295\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1129\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1131\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1132\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    500\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSocketClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mauthkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/multiprocessing/connection.py\u001b[0m in \u001b[0;36mSocketClient\u001b[0;34m(address)\u001b[0m\n\u001b[1;32m    628\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetblocking\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m         \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AxJximxrQiQY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}